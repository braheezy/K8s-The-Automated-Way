---

- hosts: aws_ec2
  become: yes
  pre_tasks:
    - set_fact:
        vm_name: "{{ hostvars[inventory_hostname]['vm_name'] }}"

    - name: Ensure system is updated
      apt:
        upgrade: yes
        update_cache: yes

    - name: Ensure tools are installed
      apt:
        name:
          - bat
          - net-tools
        state: present

- hosts: controllers
  become: yes
  tasks:
    - name: Distribute certs/keys/kubeconfigs to controllers
      copy:
        src: '{{ item }}'
        dest: '{{ ansible_env.HOME }}'
      loop:
        - pki/service-account-key.pem
        - pki/service-account.pem
        - pki/ca.pem
        - pki/ca-key.pem
        - pki/kubernetes-key.pem
        - pki/kubernetes.pem
        - kubeconfigs/admin.kubeconfig
        - kubeconfigs/kube-controller-manager.kubeconfig
        - kubeconfigs/kube-scheduler.kubeconfig
        - encryption-config.yml

    - import_role:
        name: etcd
    - import_role:
        name: control_plane

- hosts: localhost
  connection: local
  tasks:
    - name: Do external health check
      uri:
        url: https://{{ k8s_public_ip }}:6443/version
        ca_path: pki/ca.pem
      register: result

    - debug:
        var: result.json

- hosts: controllers[0]
  become: yes
  tasks:
    - name: Copy in RBAC configs
      copy:
        src: rbac/{{ item }}
        dest: /tmp/
      loop:
        - ClusterRole.yml
        - ClusterRoleBinding.yml

    - name: Create the system:kube-apiserver-to-kubelet ClusterRole
      command: kubectl apply --kubeconfig {{ ansible_env.HOME }}/admin.kubeconfig -f /tmp/ClusterRole.yml

    - name: Bind the system:kube-apiserver-to-kubelet ClusterRole to the kubernetes user
      command: kubectl apply --kubeconfig {{ ansible_env.HOME }}/admin.kubeconfig -f /tmp/ClusterRoleBinding.yml

- hosts: workers
  become: yes
  tasks:
    - name: Distribute certs/keys/kubeconfigs to workers
      copy:
        src: '{{ item }}'
        dest: '{{ ansible_env.HOME }}'
      loop:
        - pki/{{ vm_name }}-key.pem
        - pki/{{ vm_name }}.pem
        - pki/ca.pem
        - kubeconfigs/{{ vm_name }}.kubeconfig
        - kubeconfigs/kube-proxy.kubeconfig

    - import_role:
        name: node

- hosts: controllers[0]
  become: yes
  tasks:
    - name: Check nodes are running
      command: kubectl get nodes --kubeconfig {{ ansible_env.HOME }}/admin.kubeconfig
      register: result

    - debug:
        var: result.stdout

- hosts: localhost
  connection: local
  tasks:
    - name: Configure admin remote kubectl access
      shell: |
        config set-cluster kubernetes-the-hard-way \
          --certificate-authority=pki/ca.pem \
          --embed-certs=true \
          --server=https://{{ k8s_public_ip }}:6443

        kubectl config set-cluster kubernetes-the-hard-way \
          --certificate-authority=pki/ca.pem \
          --embed-certs=true \
          --server=https://{{ k8s_public_ip }}:6443

        kubectl config set-credentials admin \
          --client-certificate=pki/admin.pem \
          --client-key=pki/admin-key.pem

        kubectl config set-context kubernetes-the-hard-way \
          --cluster=kubernetes-the-hard-way \
          --user=admin

        kubectl config use-context kubernetes-the-hard-way

    - name: Configure CoreDNS on cluster, giving apps in the cluster DNS names
      command: kubectl apply -f deployments/coredns.yml

    - name: Confirm basic functionality of cluster!
      command: kubectl get pods -l k8s-app=kube-dns -n kube-system
      register: result

    - debug:
        var: result.stdout